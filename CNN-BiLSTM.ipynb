{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Titulo de ejemplo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation dataframe: 413 positives, 382 negatives (0.519496855345912%)\n",
      "Test dataframe: 1652 positives, 1526 negatives (0.5198237885462555%)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "SEED = 0\n",
    "FRAC_VAL = 0.2\n",
    "\n",
    "train_df = pd.read_csv('./datasets/all_veltri_2.csv', index_col = 0).sample(frac=1, random_state=SEED)\n",
    "df_pos = pd.read_csv('./datasets/veltri_dramp_cdhit_90_2.csv', index_col = 0)\n",
    "df_neg = pd.read_csv('./datasets/non_amp_ampep_cdhit90_2.csv', index_col = 0)\n",
    "\n",
    "val_df_pos = df_pos.sample(frac=FRAC_VAL, random_state=SEED)\n",
    "val_df_neg = df_neg.sample(frac=FRAC_VAL, random_state=SEED)\n",
    "test_df_pos = df_pos.drop(val_df_pos.index)\n",
    "test_df_neg = df_neg.drop(val_df_neg.index)\n",
    "\n",
    "val_df = pd.concat([val_df_pos, val_df_neg]).sample(frac=1, random_state=SEED)\n",
    "test_df = pd.concat([test_df_pos, test_df_neg]).sample(frac=1, random_state=SEED)\n",
    "\n",
    "print(f\"Validation dataframe: {len(val_df_pos)} positives, {len(val_df_neg)} negatives ({len(val_df_pos)/len(val_df)}%)\")\n",
    "print(f\"Test dataframe: {len(test_df_pos)} positives, {len(test_df_neg)} negatives ({len(test_df_pos)/len(test_df)}%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Funciones random go BRRRR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import BertTokenizer\n",
    "from time import process_time_ns \n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support, confusion_matrix\n",
    "from pandas import DataFrame\n",
    "from itertools import product\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from copy import deepcopy\n",
    "from torch.optim import AdamW\n",
    "from transformers import get_linear_schedule_with_warmup\n",
    "import pandas as pd\n",
    "from time import process_time_ns\n",
    "import torch\n",
    "from pipeline_tools import compute_loss\n",
    "\n",
    "class AMP_BioChemLLDataset(Dataset):\n",
    "    \"\"\"\n",
    "        Esta clase permite formar un Dataset legible para los modelos de PyTorch\n",
    "        Implementa los métodos necesarios para entrenar un BERT\n",
    "    \"\"\"\n",
    "    def __init__(self, df, biochem_cols, tokenizer_name='Rostlab/prot_bert_bfd', max_len=200):\n",
    "        super(Dataset, AMP_BioChemLLDataset).__init__(self)\n",
    "        self.tokenizer = BertTokenizer.from_pretrained(tokenizer_name, do_lower_case=False)\n",
    "        self.df = df\n",
    "        self.max_len = max_len\n",
    "        \n",
    "        self.seqs = list(df['aa_seq'])\n",
    "        self.biochem = df[biochem_cols]\n",
    "        self.labels = list(df['AMP'].astype(int))\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        seq = \" \".join(\"\".join(self.seqs[idx].split()))\n",
    "        seq_enc = self.tokenizer(\n",
    "            seq, \n",
    "            truncation=True, \n",
    "            padding='max_length', \n",
    "            max_length=self.max_len,\n",
    "            return_tensors = 'pt',\n",
    "            return_attention_mask=True\n",
    "        )\n",
    "        seq_label = self.labels[idx]\n",
    "        seq_biochem = self.biochem.iloc[idx]\n",
    "        seq_biochem.loc['molecular_mass'] = seq_biochem['molecular_mass'] / 1e4\n",
    "        seq_biochem.transpose()\n",
    "        \n",
    "        return {\n",
    "            'idx': idx,\n",
    "            'input_ids': seq_enc['input_ids'].flatten(),\n",
    "            'attention_mask' : seq_enc['attention_mask'].flatten(),\n",
    "            'labels' : torch.tensor(seq_label, dtype=torch.long),\n",
    "            'biochem_info': torch.tensor(seq_biochem, dtype=torch.float32),\n",
    "            'zscore': torch.rand(150, 133)\n",
    "        }\n",
    "    \n",
    "\n",
    "class AMP_BioChemLLDataLoader(DataLoader):\n",
    "    \"\"\"\n",
    "        Es una estructura de datos iterable con mini-batches de datos\n",
    "    \n",
    "        dataframe   --  Un dataframe de Pandas con los datos, con columnas 'aa_seq' y 'AMP'\n",
    "        batch_size  --  El tamaño de mini-batch con el que vas a entrenar el modelo   \n",
    "    \"\"\"\n",
    "    def __init__(self, dataframe, biochem_cols, batch_size):\n",
    "        DataLoader.__init__(\n",
    "            self,\n",
    "            AMP_BioChemLLDataset(dataframe, biochem_cols),\n",
    "            batch_size = batch_size,\n",
    "            num_workers = 2,\n",
    "            shuffle = True\n",
    "        )\n",
    "        \n",
    "def train_biochemLL_model(model, data_loader, loss_fn, optimizer, scheduler, verbose = False):\n",
    "    \"\"\"\n",
    "        Entrena un modelo, y devuelve etiquetas reales, predicciones y el loss final\n",
    "        \n",
    "        model         -- El modelo a entrenar\n",
    "        data_loader   -- un dataloader con los ejemplos de entrenamiento\n",
    "        loss_fn       -- La funcion de loss (MSE, CrossEntropy, etc.)\n",
    "        optimizer     -- El optimizador del modelo\n",
    "        scheduler     -- El scheduler del learning rate del optimizador\n",
    "        verbose       -- True para mostrar informacion del entrenamiento por consola\n",
    "    \"\"\"\n",
    "    \n",
    "    model = model.train() # Explicitly setting model to train state\n",
    "    labels = []\n",
    "    predictions = []\n",
    "    losses = []\n",
    "    correct_predictions = 0\n",
    "    \n",
    "    # Variables para calcular una media del loss (no afecta al entrenamiento)\n",
    "    mobile_loss = 0\n",
    "    MOBILE_COEF = 0.9\n",
    "    \n",
    "    i = 0\n",
    "    for d in data_loader:\n",
    "        # Medimos el tiempo\n",
    "        i = i + 1\n",
    "        start = process_time_ns()\n",
    "\n",
    "        # Obtenemos los atributos del siguiente batch\n",
    "        input_ids = d['input_ids'].to(\"cuda:0\")\n",
    "        attention_mask = d['attention_mask'].to(\"cuda:0\")\n",
    "        biochem_info = d['biochem_info'].to('cuda:0')\n",
    "        targets = d['labels'].to(\"cuda:0\")\n",
    "        zscore = d['zscore'].to(\"cuda:0\")\n",
    "        \n",
    "        # Lo usamos como input para el modelo y obtenemos el output\n",
    "        outputs = model(zscore = zscore, input_ids = input_ids, attention_mask = attention_mask, biochem_info = biochem_info)\n",
    "\n",
    "        # La predicción es la clase con mayor logit\n",
    "        preds = torch.argmax(outputs.logits, dim = 1)\n",
    "                \n",
    "        # Guardamos la prediccion y la etiqueta real, para luego calcular metricas\n",
    "        labels += targets.tolist()\n",
    "        predictions += preds.tolist()\n",
    "                \n",
    "        # Calculamos el loss\n",
    "        loss = compute_loss(loss_fn, outputs, targets)\n",
    "        losses.append(loss.item())\n",
    "        \n",
    "        # Calculamos la media movil del loss\n",
    "        mobile_loss = MOBILE_COEF*mobile_loss + (1-MOBILE_COEF)*loss.item()\n",
    "        \n",
    "        # Hacemos el backprop\n",
    "        loss.backward()\n",
    "        \n",
    "        # Clip the gradients of the model to prevent exploding gradients using clip_grad_norm\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm = 1.0)\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Medimos de nuevo\n",
    "        end = process_time_ns()\n",
    "        step_time = (end - start) // (10 ** 6)\n",
    "        remaining_min = (step_time*(len(data_loader) - i) // (10 ** 3)) // 60\n",
    "        remaining_sec = (step_time*(len(data_loader) - i) // (10 ** 3)) - remaining_min * 60\n",
    "\n",
    "        # Imprimimos si es necesario\n",
    "        if verbose:\n",
    "            if i % 10 == 0:\n",
    "                print(f\"Step {i}/{len(data_loader)}: Loss (avg) {mobile_loss}, Step Time {step_time} ms, ETA {remaining_min}:{remaining_sec}\")\n",
    "\n",
    "    return labels, predictions, losses\n",
    "\n",
    "\n",
    "def eval_biochemLL_model(model, data_loader, loss_fn, verbose = False):\n",
    "    \"\"\"\n",
    "        Evalua un modelo con un conjunto de datos de test\n",
    "        \n",
    "        model         -- El modelo a entrenar\n",
    "        data_loader   -- un dataloader con los ejemplos de entrenamiento\n",
    "        loss_fn       -- La funcion de loss (MSE, CrossEntropy, etc.)\n",
    "        verbose       -- True para mostrar informacion del entrenamiento por consola\n",
    "    \"\"\"\n",
    "    model = model.eval()\n",
    "    labels = []\n",
    "    predictions = []\n",
    "    \n",
    "    # Variables para calcular una media del loss (no afecta al entrenamiento)\n",
    "    mobile_loss = 0\n",
    "    MOBILE_COEF = 0.9\n",
    "\n",
    "    with torch.no_grad():\n",
    "        i = 0\n",
    "        for d in data_loader:\n",
    "            # Medimos el tiempo\n",
    "            i = i + 1\n",
    "            start = process_time_ns()\n",
    "\n",
    "            # Obtenemos los atributos del siguiente batch\n",
    "            input_ids = d['input_ids'].to(\"cuda:0\")\n",
    "            attention_mask = d['attention_mask'].to(\"cuda:0\")\n",
    "            biochem_info = d['biochem_info'].to('cuda:0')\n",
    "            targets = d['labels'].to(\"cuda:0\")\n",
    "        \n",
    "            # Lo usamos como input para el modelo y obtenemos el output\n",
    "            outputs = model(input_ids = input_ids, attention_mask = attention_mask, biochem_info = biochem_info)\n",
    "\n",
    "            # La predicción es la clase con mayor logit\n",
    "            preds = torch.argmax(outputs.logits, dim = 1)\n",
    "                \n",
    "            # Guardamos la prediccion y la etiqueta real, para luego calcular metricas\n",
    "            labels += targets.tolist()\n",
    "            predictions += preds.tolist()\n",
    "            \n",
    "            # Calculamos el loss\n",
    "            loss = compute_loss(loss_fn, outputs, targets)\n",
    "            \n",
    "            # Calculamos la media movil del loss\n",
    "            mobile_loss = MOBILE_COEF*mobile_loss + (1-MOBILE_COEF)*loss.item()\n",
    "            \n",
    "            # Medimos de nuevo\n",
    "            end = process_time_ns()\n",
    "            step_time = (end - start) // (10 ** 6)\n",
    "            remaining_min = (step_time*(len(data_loader) - i) // (10 ** 3)) // 60\n",
    "            remaining_sec = (step_time*(len(data_loader) - i) // (10 ** 3)) - remaining_min * 60\n",
    "    \n",
    "            # Imprimimos si es necesario\n",
    "            if verbose:\n",
    "                if i % 10 == 0:\n",
    "                    print(f\"Step {i}/{len(data_loader)}: Loss (avg) {mobile_loss}, Step Time {step_time} ms, ETA {remaining_min}:{remaining_sec}\")\n",
    "\n",
    "    return labels, predictions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at Rostlab/prot_bert_bfd were not used when initializing BertForSequenceClassification: ['cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight', 'cls.predictions.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.bias', 'cls.predictions.transform.LayerNorm.bias']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at Rostlab/prot_bert_bfd and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/usr/local/anaconda3/lib/python3.7/site-packages/pandas/core/indexing.py:1637: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  self._setitem_single_block(indexer, value, name)\n",
      "/usr/local/anaconda3/lib/python3.7/site-packages/pandas/core/indexing.py:692: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  iloc._setitem_with_indexer(indexer, value, self.name)\n",
      "/usr/local/anaconda3/lib/python3.7/site-packages/pandas/core/indexing.py:1637: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  self._setitem_single_block(indexer, value, name)\n",
      "/usr/local/anaconda3/lib/python3.7/site-packages/pandas/core/indexing.py:692: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  iloc._setitem_with_indexer(indexer, value, self.name)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 10/445: Loss (avg) 0.45738496044937726, Step Time 705 ms, ETA 5:6\n",
      "Step 20/445: Loss (avg) 0.6204272236780454, Step Time 715 ms, ETA 5:3\n",
      "Step 30/445: Loss (avg) 0.6674338246557026, Step Time 714 ms, ETA 4:56\n",
      "Step 40/445: Loss (avg) 0.6670646581167752, Step Time 717 ms, ETA 4:50\n",
      "Step 50/445: Loss (avg) 0.6501221942685821, Step Time 721 ms, ETA 4:44\n",
      "Step 60/445: Loss (avg) 0.6389096301645191, Step Time 722 ms, ETA 4:37\n",
      "Step 70/445: Loss (avg) 0.5979992179602636, Step Time 720 ms, ETA 4:30\n",
      "Step 80/445: Loss (avg) 0.5616963876072636, Step Time 721 ms, ETA 4:23\n",
      "Step 90/445: Loss (avg) 0.568687351922433, Step Time 724 ms, ETA 4:17\n",
      "Step 100/445: Loss (avg) 0.5231240741537585, Step Time 724 ms, ETA 4:9\n",
      "Step 110/445: Loss (avg) 0.5165157187966093, Step Time 725 ms, ETA 4:2\n",
      "Step 120/445: Loss (avg) 0.5109281807616192, Step Time 726 ms, ETA 3:55\n",
      "Step 130/445: Loss (avg) 0.487256991140266, Step Time 728 ms, ETA 3:49\n",
      "Step 140/445: Loss (avg) 0.4837104464045119, Step Time 726 ms, ETA 3:41\n",
      "Step 150/445: Loss (avg) 0.44808358257243064, Step Time 726 ms, ETA 3:34\n",
      "Step 160/445: Loss (avg) 0.47362253364508633, Step Time 728 ms, ETA 3:27\n",
      "Step 170/445: Loss (avg) 0.3826952933969644, Step Time 727 ms, ETA 3:19\n",
      "Step 180/445: Loss (avg) 0.3407425918301757, Step Time 728 ms, ETA 3:12\n",
      "Step 190/445: Loss (avg) 0.3635697214503049, Step Time 728 ms, ETA 3:5\n",
      "Step 200/445: Loss (avg) 0.36417263125504623, Step Time 728 ms, ETA 2:58\n",
      "Step 210/445: Loss (avg) 0.3258317588055787, Step Time 727 ms, ETA 2:50\n",
      "Step 220/445: Loss (avg) 0.2921055464429568, Step Time 729 ms, ETA 2:44\n",
      "Step 230/445: Loss (avg) 0.35100310274362256, Step Time 732 ms, ETA 2:37\n",
      "Step 240/445: Loss (avg) 0.28553896356251807, Step Time 730 ms, ETA 2:29\n",
      "Step 250/445: Loss (avg) 0.28451074260260284, Step Time 731 ms, ETA 2:22\n",
      "Step 260/445: Loss (avg) 0.2960164778840003, Step Time 729 ms, ETA 2:14\n",
      "Step 270/445: Loss (avg) 0.3726421357392032, Step Time 730 ms, ETA 2:7\n",
      "Step 280/445: Loss (avg) 0.31764902666912104, Step Time 729 ms, ETA 2:0\n",
      "Step 290/445: Loss (avg) 0.31921527977244135, Step Time 729 ms, ETA 1:52\n",
      "Step 300/445: Loss (avg) 0.32622696689443337, Step Time 731 ms, ETA 1:45\n",
      "Step 310/445: Loss (avg) 0.31090806373020563, Step Time 731 ms, ETA 1:38\n",
      "Step 320/445: Loss (avg) 0.34642920329387084, Step Time 731 ms, ETA 1:31\n",
      "Step 330/445: Loss (avg) 0.28550142686839114, Step Time 729 ms, ETA 1:23\n",
      "Step 340/445: Loss (avg) 0.2525264859842553, Step Time 729 ms, ETA 1:16\n",
      "Step 350/445: Loss (avg) 0.2641821148712097, Step Time 729 ms, ETA 1:9\n",
      "Step 360/445: Loss (avg) 0.25462566458502545, Step Time 730 ms, ETA 1:2\n",
      "Step 370/445: Loss (avg) 0.2673922442945589, Step Time 730 ms, ETA 0:54\n",
      "Step 380/445: Loss (avg) 0.3038473067754669, Step Time 730 ms, ETA 0:47\n",
      "Step 390/445: Loss (avg) 0.22184334122297855, Step Time 736 ms, ETA 0:40\n",
      "Step 400/445: Loss (avg) 0.33214984867787606, Step Time 731 ms, ETA 0:32\n",
      "Step 410/445: Loss (avg) 0.266716903713229, Step Time 730 ms, ETA 0:25\n",
      "Step 420/445: Loss (avg) 0.3686696276637852, Step Time 731 ms, ETA 0:18\n",
      "Step 430/445: Loss (avg) 0.2971266583803864, Step Time 731 ms, ETA 0:10\n",
      "Step 440/445: Loss (avg) 0.24933254597306034, Step Time 733 ms, ETA 0:3\n",
      "Metrics for train set: \n",
      "   accuracy        f1  precision    recall  specificity     auroc      aupr  \\\n",
      "0  0.853768  0.857378   0.836724  0.879078     0.828459  0.853768  0.888131   \n",
      "\n",
      "             confusion_matrix  \n",
      "0  [[1473, 305], [215, 1563]]  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/anaconda3/lib/python3.7/site-packages/pandas/core/indexing.py:1637: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  self._setitem_single_block(indexer, value, name)\n",
      "/usr/local/anaconda3/lib/python3.7/site-packages/pandas/core/indexing.py:692: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  iloc._setitem_with_indexer(indexer, value, self.name)\n",
      "/usr/local/anaconda3/lib/python3.7/site-packages/pandas/core/indexing.py:1637: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  self._setitem_single_block(indexer, value, name)\n",
      "/usr/local/anaconda3/lib/python3.7/site-packages/pandas/core/indexing.py:692: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  iloc._setitem_with_indexer(indexer, value, self.name)\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "sum(): argument 'input' (position 1) must be Tensor, not NoneType",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-6dd993061e00>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     51\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m \u001b[0;31m# Obtener las métricas de validacion\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 53\u001b[0;31m \u001b[0mtest_labels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_preds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0meval_biochemLL_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmulti_gpu_bert\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_dataloader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mCrossEntropyLoss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     54\u001b[0m \u001b[0mtest_metrics\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompute_metrics\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_labels\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_preds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     55\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-2-a7d620cdd6ab>\u001b[0m in \u001b[0;36meval_biochemLL_model\u001b[0;34m(model, data_loader, loss_fn, verbose)\u001b[0m\n\u001b[1;32m    179\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    180\u001b[0m             \u001b[0;31m# Lo usamos como input para el modelo y obtenemos el output\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 181\u001b[0;31m             \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_ids\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput_ids\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattention_mask\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mattention_mask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbiochem_info\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbiochem_info\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    182\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    183\u001b[0m             \u001b[0;31m# La predicción es la clase con mayor logit\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1192\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1195\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1196\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Escritorio/MyGit/AMP-BERT-BIOCHEM/MultiGPUModels.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, labels, output_attentions, output_hidden_states, return_dict, biochem_info, zscore)\u001b[0m\n\u001b[1;32m    407\u001b[0m             \u001b[0mlogits\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclassifier\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput_with_biochem\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    408\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 409\u001b[0;31m             \u001b[0mlogits\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlogits\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m0.00000001\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mzscore\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    410\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    411\u001b[0m             return SequenceClassifierOutput(\n",
      "\u001b[0;31mTypeError\u001b[0m: sum(): argument 'input' (position 1) must be Tensor, not NoneType"
     ]
    }
   ],
   "source": [
    "from pipeline_tools import AMP_BioChemDataLoader, train_biochem_model, eval_biochem_model, compute_metrics\n",
    "from torch.optim import AdamW\n",
    "from transformers import BertForSequenceClassification, get_linear_schedule_with_warmup\n",
    "from MultiGPUModels import MultiGPUBertForPeptideLLClassification\n",
    "from torch.nn import CrossEntropyLoss\n",
    "\n",
    "BATCH_SIZE = 8\n",
    "LEARNING_RATE = 5e-6\n",
    "WEIGHT_DECAY = 0.01\n",
    "EPOCHS = 1\n",
    "            \n",
    "biochem_cols = [\n",
    "    \"molecular_mass\",\n",
    "    \"hydrophobic_freq\",\n",
    "    \"hydrophilic_freq\",\n",
    "    \"basic_freq\",\n",
    "    \"acid_freq\",\n",
    "    \"charge\",\n",
    "    \"aliphatic_index\",\n",
    "    \"average_hydrophobicity\",\n",
    "    \"isoelectric_point\"\n",
    "]\n",
    "\n",
    "train_dataloader = AMP_BioChemLLDataLoader(train_df, biochem_cols, batch_size = BATCH_SIZE)            \n",
    "test_dataloader = AMP_BioChemLLDataLoader(test_df, biochem_cols, batch_size = BATCH_SIZE)            \n",
    "\n",
    "# Copiar el modelo para entrenarlo\n",
    "bert_model = BertForSequenceClassification.from_pretrained('Rostlab/prot_bert_bfd')   \n",
    "multi_gpu_bert = MultiGPUBertForPeptideLLClassification(bert_model, biochem_cols)\n",
    "            \n",
    "# Entrenar el modelo con esta configuracion\n",
    "optimizer = AdamW(\n",
    "    multi_gpu_bert.parameters(), \n",
    "    lr = LEARNING_RATE, \n",
    "    weight_decay = WEIGHT_DECAY)\n",
    "            \n",
    "total_steps = len(train_dataloader) * EPOCHS\n",
    "            \n",
    "scheduler = get_linear_schedule_with_warmup(optimizer, \n",
    "    num_warmup_steps = 0, \n",
    "    num_training_steps = total_steps)\n",
    "\n",
    "for i in range(EPOCHS):\n",
    "    labels, predicted, _ = train_biochemLL_model(multi_gpu_bert, train_dataloader, CrossEntropyLoss(), optimizer, scheduler, True)\n",
    "\n",
    "# Obtener las métricas de entrenamiento\n",
    "train_metrics = compute_metrics(labels, predicted)\n",
    "            \n",
    "print(f\"Metrics for train set: \")\n",
    "print(train_metrics)\n",
    "\n",
    "# Obtener las métricas de validacion\n",
    "test_labels, test_preds = eval_biochemLL_model(multi_gpu_bert, test_dataloader, CrossEntropyLoss(), True)\n",
    "test_metrics = compute_metrics(test_labels, test_preds)\n",
    "            \n",
    "print(f\"Metrics for test set: \")\n",
    "print(test_metrics)\n",
    "\n",
    "train_metrics.to_csv('./biochem_results/train_metrics_with_biochem_LL_1ep.csv')\n",
    "test_metrics.to_csv('./biochem_results/test_metrics_with_biochem_LL_1ep.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class CNNBiLSTM(nn.Module):\n",
    "    def __init__(self, num_preds):\n",
    "        r\"\"\"\n",
    "            Copia un encoder, dividiendolo en dos partes, que van a sendas GPUs.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.conv = torch.nn.Conv1d(\n",
    "            in_channels = num_preds, \n",
    "            out_channels = 512,\n",
    "            kernel_size = 100\n",
    "        )\n",
    "        self.lstm = torch.nn.LSTM(\n",
    "            input_size = 512,\n",
    "            hidden_size = 512,\n",
    "            bidirectional = True\n",
    "        )\n",
    "        \n",
    "        \n",
    "    def forward(\n",
    "        self,\n",
    "        aa_preds: torch.Tensor,\n",
    "    ) -> torch.Tensor:\n",
    "    \n",
    "        output = self.conv(aa_preds)\n",
    "        output = output.transpose(0,1)\n",
    "        output = self.lstm(output)\n",
    "        \n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "a = torch.tensor([1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "mymodel = CNNBiLSTM(133).to(\"cuda:1\")\n",
    "\n",
    "samples = [torch.rand(133,200) for i in range(20000)]\n",
    "\n",
    "for i in range(20000):\n",
    "    mymodel(samples[i].to(\"cuda:1\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'BertForSequenceClassification' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-e890b3c8e160>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mclass\u001b[0m \u001b[0mMultiGPUBertForPeptideAAClassification\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mBertForSequenceClassification\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbert_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbiochem_global_cols\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbiochem_aa_cols\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mMultiGPUBertForPeptideAAClassification\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbert_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_labels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbert_model_for_sequence_classification\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_labels\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'BertForSequenceClassification' is not defined"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import BertForSequenceClassification\n",
    "from MultiGPUModels import MultiGPUBertModel\n",
    "\n",
    "class MultiGPUBertForPeptideAAClassification(torch.nn.Module):\n",
    "    def __init__(self, bert_model_for_class, biochem_global_cols, biochem_aa_cols):\n",
    "        super().__init__()\n",
    "                \n",
    "        self.num_labels = bert_model_for_class.num_labels\n",
    "\n",
    "        self.bert = MultiGPUBertModel(bert_model_for_class.bert)\n",
    "        self.bert_dropout = nn.Dropout(0.01).to(\"cuda:0\")\n",
    "        \n",
    "        self.biochem_aa_cols = biochem_aa_cols\n",
    "        self.cnn_rnn = nn.CNNBiLSTM(len(biochem_aa_cols)).to(\"cuda:1\")\n",
    "        self.cnn_rnn_dropout = nn.Dropout(0.01).to(\"cuda:1\")\n",
    "\n",
    "        self.biochem_global_cols = biochem_global_cols\n",
    "        \n",
    "        self.classifier = nn.Linear(\n",
    "            self.config.hidden_size + len(self.biochem_global_cols) + len(self.biochem_aa_cols), \n",
    "            self.num_labels\n",
    "        ).to(\"cuda:0\")\n",
    "        \n",
    "    def forward(\n",
    "            self,\n",
    "            input_ids: Optional[torch.Tensor] = None,\n",
    "            attention_mask: Optional[torch.Tensor] = None,\n",
    "            biochem_global_info: torch.Tensor = None,\n",
    "            biochem_aa_info: torch.Tensor = None,\n",
    "        ) -> SequenceClassifierOutput:\n",
    "            r\"\"\"\n",
    "                Se entrena por un lado el Encoder, con los input_ids, as attention_mask y todo eso\n",
    "                \n",
    "                Por otro lado, la BiLSTM se entrena con la informacion por aminoácido\n",
    "                \n",
    "                Finalmente se agregan los hidden_states de ambos componentes junto con los predictores\n",
    "                globales para clasificar y obtener las probabilidades para cada clase\n",
    "            \"\"\"\n",
    "    \n",
    "            # Primera parte: embeddings del encoder\n",
    "    \n",
    "            # Obtengo los embeddings generados por el encoder \n",
    "            bert_outputs = self.bert(\n",
    "                input_ids,\n",
    "                attention_mask=attention_mask,\n",
    "            )\n",
    "            bert_pooled_output = outputs[1]\n",
    "    \n",
    "            # Aplico dropout sobre este embedding\n",
    "            bert_dropout_output = self.dropout(bert_pooled_output)\n",
    "            \n",
    "            \n",
    "            # Segunda parte: cell info del CNN_BiLSTM\n",
    "        \n",
    "            # Proceso los predictores por aminoácido\n",
    "            cnn_rnn_output = self.cnn_rnn(biochem_aa_info)\n",
    "            \n",
    "            # Me quedo con el cell_state, que representa toda la secuencia\n",
    "            cell_state = cnn_rnn_output[1][1]\n",
    "            \n",
    "            # De nuevo aplico dropout\n",
    "            cnn_rnn_dropout_output = self.cnn_rnn_dropout(cell_state)\n",
    "            \n",
    "            \n",
    "            # Concateno toda la información\n",
    "            output_with_biochem = torch.cat([\n",
    "                bert_dropout_output,\n",
    "                cnn_rnn_dropout_output, \n",
    "                biochem_global_info\n",
    "            ], dim = 1)\n",
    "            \n",
    "            # Clasifico \n",
    "            logits = self.classifier(output_with_biochem)\n",
    "            \n",
    "            return SequenceClassifierOutput(\n",
    "                loss=None,\n",
    "                logits=logits,\n",
    "                hidden_states=pooled_output,\n",
    "                attentions=outputs.attentions,\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AMP_BioChemDataset(Dataset):\n",
    "    \"\"\"\n",
    "        Esta clase permite formar un Dataset legible para los modelos de PyTorch\n",
    "        Implementa los métodos necesarios para entrenar un BERT\n",
    "    \"\"\"\n",
    "    def __init__(self, df, biochem_cols, tokenizer_name='Rostlab/prot_bert_bfd', max_len=200):\n",
    "        super(Dataset, AMP_BioChemDataset).__init__(self)\n",
    "        self.tokenizer = BertTokenizer.from_pretrained(tokenizer_name, do_lower_case=False)\n",
    "        self.df = df\n",
    "        self.max_len = max_len\n",
    "        \n",
    "        self.seqs = list(df['aa_seq'])\n",
    "        self.biochem = df[biochem_cols]\n",
    "        self.labels = list(df['AMP'].astype(int))\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        seq = \" \".join(\"\".join(self.seqs[idx].split()))\n",
    "        seq_enc = self.tokenizer(\n",
    "            seq, \n",
    "            truncation=True, \n",
    "            padding='max_length', \n",
    "            max_length=self.max_len,\n",
    "            return_tensors = 'pt',\n",
    "            return_attention_mask=True\n",
    "        )\n",
    "        seq_label = self.labels[idx]\n",
    "        seq_biochem = self.biochem.iloc[idx]\n",
    "        seq_biochem.loc['molecular_mass'] = seq_biochem['molecular_mass'] / 1e4\n",
    "        seq_biochem.transpose()\n",
    "        \n",
    "        return {\n",
    "            'idx': idx,\n",
    "            'input_ids': seq_enc['input_ids'].flatten(),\n",
    "            'attention_mask' : seq_enc['attention_mask'].flatten(),\n",
    "            'labels' : torch.tensor(seq_label, dtype=torch.long),\n",
    "            'biochem_info': torch.tensor(seq_biochem, dtype=torch.float32)\n",
    "        }\n",
    "    \n",
    "\n",
    "class AMP_BioChemDataLoader(DataLoader):\n",
    "    \"\"\"\n",
    "        Es una estructura de datos iterable con mini-batches de datos\n",
    "    \n",
    "        dataframe   --  Un dataframe de Pandas con los datos, con columnas 'aa_seq' y 'AMP'\n",
    "        batch_size  --  El tamaño de mini-batch con el que vas a entrenar el modelo   \n",
    "    \"\"\"\n",
    "    def __init__(self, dataframe, biochem_cols, batch_size):\n",
    "        DataLoader.__init__(\n",
    "            self,\n",
    "            AMP_BioChemDataset(dataframe, biochem_cols),\n",
    "            batch_size = batch_size,\n",
    "            num_workers = 2,\n",
    "            shuffle = True\n",
    "        )\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

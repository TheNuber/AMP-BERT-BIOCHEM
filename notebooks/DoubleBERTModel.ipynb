{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Double BERT Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from typing import Optional\n",
    "from transformers import BertForSequenceClassification, BertModel, BertConfig\n",
    "from transformers.modeling_outputs import SequenceClassifierOutput\n",
    "from MultiGPUModels import MultiGPUBertModel\n",
    "\n",
    "class MultiGPUDoubleBertForPeptideClassification(torch.nn.Module):\n",
    "    def __init__(self, bert_model_for_class, biochem_global_cols):\n",
    "        super().__init__()\n",
    "                \n",
    "        self.num_labels = bert_model_for_class.num_labels\n",
    "\n",
    "        self.raw_seq_bert = MultiGPUBertModel(bert_model_for_class.bert)\n",
    "        self.raw_seq_dropout = nn.Dropout(0.01).to(\"cuda:0\")\n",
    "        \n",
    "        self.aa_encoding_bert_config = BertConfig(\n",
    "            vocab_size = 30522, \n",
    "            hidden_size = 133, \n",
    "            num_hidden_layers = 2,\n",
    "            num_attention_heads = 19,\n",
    "            intermediate_size = 1024,\n",
    "            hidden_act = 'gelu',\n",
    "            hidden_dropout_prob = 0.1,\n",
    "            attention_probs_dropout_prob = 0.1,\n",
    "            max_position_embeddings = 512,\n",
    "            type_vocab_size = 2,\n",
    "            initializer_range = 0.02,\n",
    "            layer_norm_eps = 1e-12,\n",
    "            pad_token_id = 0,\n",
    "            position_embedding_type = 'absolute',\n",
    "            use_cache = True,\n",
    "            classifier_dropout = None \n",
    "        )\n",
    "        \n",
    "        self.aa_encoding_bert = BertModel(self.aa_encoding_bert_config).to(\"cuda:1\")\n",
    "        self.aa_encoding_dropout = nn.Dropout(0.01).to(\"cuda:1\")\n",
    "\n",
    "        self.biochem_global_cols = biochem_global_cols\n",
    "        \n",
    "        self.classifier = nn.Linear(\n",
    "            bert_model_for_class.config.hidden_size + len(self.biochem_global_cols) + self.aa_encoding_bert_config.hidden_size, \n",
    "            self.num_labels\n",
    "        ).to(\"cuda:0\")\n",
    "        \n",
    "    def forward(\n",
    "            self,\n",
    "            dataloader_item = None,\n",
    "            input_ids: Optional[torch.Tensor] = None,\n",
    "            attention_mask: Optional[torch.Tensor] = None,\n",
    "            aa_encoding: Optional[torch.Tensor] = None,\n",
    "            biochem_global_info: torch.Tensor = None,\n",
    "        ) -> SequenceClassifierOutput:\n",
    "            r\"\"\"\n",
    "                Se entrena por un lado el Encoder, con los input_ids, as attention_mask y todo eso\n",
    "                                \n",
    "                Finalmente se agregan los hidden_states de ambos componentes junto con los predictores\n",
    "                globales para clasificar y obtener las probabilidades para cada clase\n",
    "            \"\"\"\n",
    "            \n",
    "            if dataloader_item != None:\n",
    "                input_ids = dataloader_item['input_ids'].to(\"cuda:0\")\n",
    "                attention_mask = dataloader_item['attention_mask'].to(\"cuda:0\") \n",
    "                aa_encoding = dataloader_item['aa_encoding'].to(\"cuda:1\") \n",
    "                biochem_global_info = dataloader_item['biochem_global_info'].to(\"cuda:0\") \n",
    "    \n",
    "            # Primera parte: embeddings del encoder para las secuencias en bruto\n",
    "    \n",
    "            # Obtengo los embeddings generados por el encoder \n",
    "            raw_seq_bert_outputs = self.raw_seq_bert(\n",
    "                input_ids=input_ids,\n",
    "                attention_mask=attention_mask,\n",
    "            )\n",
    "            raw_seq_bert_pooled_output = raw_seq_bert_outputs[1]\n",
    "    \n",
    "            # Aplico dropout sobre este embedding\n",
    "            raw_seq_bert_dropout_output = self.raw_seq_dropout(raw_seq_bert_pooled_output)\n",
    "            \n",
    "            \n",
    "            # Segunda parte: embeddings del encoder para las codificaciones con aminoacidos\n",
    "        \n",
    "            # Obtengo los embeddings generados por el encoder \n",
    "            aa_encoding_bert_outputs = self.aa_encoding_bert(\n",
    "                inputs_embeds=aa_encoding,\n",
    "                attention_mask=attention_mask.to(\"cuda:1\")\n",
    "            )\n",
    "            aa_encoding_bert_pooled_output = aa_encoding_bert_outputs[1]\n",
    "    \n",
    "            # Aplico dropout sobre este embedding\n",
    "            aa_encoding_bert_dropout_output = self.aa_encoding_dropout(aa_encoding_bert_pooled_output).to(\"cuda:0\")\n",
    "            \n",
    "            \n",
    "            # Concateno toda la información\n",
    "            output_with_biochem = torch.cat([\n",
    "                raw_seq_bert_dropout_output,\n",
    "                aa_encoding_bert_dropout_output, \n",
    "                biochem_global_info\n",
    "            ], dim = 1)\n",
    "            \n",
    "            # Clasifico \n",
    "            logits = self.classifier(output_with_biochem)\n",
    "            \n",
    "            return SequenceClassifierOutput(\n",
    "                loss=None,\n",
    "                logits=logits,\n",
    "                hidden_states=raw_seq_bert_pooled_output,\n",
    "                attentions=raw_seq_bert_outputs.attentions,\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from copy import deepcopy\n",
    "\n",
    "class BioChemMapper():\n",
    "    \"\"\"\n",
    "        This class encodes a peptide into a tensor.\n",
    "        \n",
    "        Each aminoacid becomes a numerical one-dimensional tensor.\n",
    "    \"\"\"\n",
    "    def __init__(self, config_file = \"./datasets/encoding_peptides_v03.csv\"):\n",
    "        self.encoder_df = pd.read_csv(config_file)\n",
    "        self.encoder_df.set_index(\"aa_code\", inplace=True)\n",
    "        \n",
    "    def map_aa_to_encoding(self, aa):\n",
    "        if aa in self.encoder_df.index:\n",
    "            return np.array(deepcopy(self.encoder_df.loc[aa]).to_list())\n",
    "        else:\n",
    "            return np.zeros(133)\n",
    "        \n",
    "    def encode_peptide(self, seq: str):\n",
    "        \"\"\"\n",
    "            Returns a numpy bidimensional array\n",
    "        \"\"\"\n",
    "        aa_list = seq.upper().split()\n",
    "        aa_encoding = map(lambda x: self.map_aa_to_encoding(x), aa_list)\n",
    "        return np.array(list(aa_encoding))\n",
    "\n",
    "    def __call__(self, seq: str):\n",
    "        return self.encode_peptide(seq)\n",
    "    \n",
    "    def get_col_names(self):\n",
    "        return self.encoder_df.columns.to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5\n"
     ]
    }
   ],
   "source": [
    "seq = \"A B C D E\"\n",
    "print(BioChemMapper()(seq).shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import BertTokenizer\n",
    "\n",
    "class AMP_BioChemDataset(Dataset):\n",
    "    \"\"\"\n",
    "        Esta clase permite formar un Dataset legible para los modelos de PyTorch\n",
    "        Implementa los métodos necesarios para entrenar un BERT\n",
    "    \"\"\"\n",
    "    def __init__(self, df, biochem_cols, tokenizer_name='Rostlab/prot_bert_bfd', max_len=200):\n",
    "        super(Dataset, AMP_BioChemDataset).__init__(self)\n",
    "        self.tokenizer = BertTokenizer.from_pretrained(tokenizer_name, do_lower_case=False)\n",
    "        self.df = df\n",
    "        self.max_len = max_len\n",
    "        self.seqs = list(df['aa_seq'])\n",
    "        self.biochem_cols = biochem_cols\n",
    "        if \"molecular_mass\" in self.biochem_cols:\n",
    "            self.df.loc[:,'molecular_mass'] = self.df.loc[:,'molecular_mass'] / 1e4\n",
    "\n",
    "        self.labels = list(df['AMP'].astype(int))\n",
    "        self.aa_encoder = BioChemMapper()\n",
    "        \n",
    "\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        seq = \" \".join(\"\".join(self.seqs[idx].split()))\n",
    "        seq_enc = self.tokenizer(\n",
    "            seq, \n",
    "            truncation=True, \n",
    "            padding='max_length', \n",
    "            max_length=self.max_len,\n",
    "            return_tensors = 'pt',\n",
    "            return_attention_mask=True\n",
    "        )\n",
    "        seq_label = self.labels[idx]\n",
    "        seq_biochem = self.df.iloc[idx].loc[biochem_cols]\n",
    "        seq_biochem.transpose()\n",
    "        \n",
    "        encoded_seq = self.aa_encoder(seq)\n",
    "        h = encoded_seq.shape[0]\n",
    "\n",
    "        if h < self.max_len:\n",
    "            padding = np.array([np.zeros(133).reshape(-1) for i in range(h,self.max_len)])\n",
    "            aa_encoding = np.vstack((encoded_seq, padding))\n",
    "        else:\n",
    "            aa_encoding = encoded_seq[:self.max_len]\n",
    "        \n",
    "        return {\n",
    "            'idx': idx,\n",
    "            'input_ids': seq_enc['input_ids'].flatten(),\n",
    "            'attention_mask' : seq_enc['attention_mask'].flatten(),\n",
    "            'labels' : torch.tensor(seq_label, dtype=torch.long),\n",
    "            'biochem_global_info': torch.tensor(seq_biochem, dtype=torch.float32),\n",
    "            'aa_encoding': torch.tensor(aa_encoding, dtype=torch.float32)\n",
    "        }\n",
    "    \n",
    "\n",
    "class AMP_BioChemDataLoader(DataLoader):\n",
    "    \"\"\"\n",
    "        Es una estructura de datos iterable con mini-batches de datos\n",
    "    \n",
    "        dataframe   --  Un dataframe de Pandas con los datos, con columnas 'aa_seq' y 'AMP'\n",
    "        batch_size  --  El tamaño de mini-batch con el que vas a entrenar el modelo   \n",
    "    \"\"\"\n",
    "    def __init__(self, dataframe, biochem_cols, batch_size):\n",
    "        DataLoader.__init__(\n",
    "            self,\n",
    "            AMP_BioChemDataset(dataframe, biochem_cols),\n",
    "            batch_size = batch_size,\n",
    "            num_workers = 2,\n",
    "            shuffle = True\n",
    "        )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import BertTokenizer\n",
    "from time import process_time_ns \n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support, confusion_matrix\n",
    "from pandas import DataFrame\n",
    "from itertools import product\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from copy import deepcopy\n",
    "from torch.optim import AdamW\n",
    "from transformers import get_linear_schedule_with_warmup, get_inverse_sqrt_schedule\n",
    "import pandas as pd\n",
    "import torch\n",
    "\n",
    "def grid_search_early_stop(model, train_data_loader, val_data_loader, grid, batch_size, loss_fn = torch.nn.CrossEntropyLoss(), verbose = False):\n",
    "        \n",
    "    param_combinations = product(\n",
    "        grid[\"learning_rate\"],\n",
    "        grid[\"weight_decay\"],\n",
    "    )\n",
    "    \n",
    "    all_combs = []\n",
    "    all_metrics = []\n",
    "    all_losses = []\n",
    "        \n",
    "    # Calculamos todas las combinaciones con el grid de hiperparametros\n",
    "    num_combinations = 1\n",
    "    for key in grid.keys():\n",
    "        num_combinations *= len(grid[key])\n",
    "        \n",
    "    print()\n",
    "    print(f\"Number of combinations: {num_combinations}\")\n",
    "\n",
    "    for combination in param_combinations:\n",
    "        \n",
    "        # En cada combinacion entrenamos y testeamos\n",
    "        learning_rate, weight_decay = combination\n",
    "\n",
    "        print()\n",
    "        print(\"Next combination:\")\n",
    "        print(f\"learning_rate: {learning_rate}\")\n",
    "        print(f\"weight_decay: {weight_decay}\")\n",
    "        \n",
    "        # Copiamos el modelo\n",
    "        model_copy = deepcopy(model)\n",
    "        \n",
    "        \n",
    "        \n",
    "        # Entrenamos hasta la mejor época\n",
    "        stop_training = False\n",
    "        epochs = 0\n",
    "        prev_f1 = 0.0\n",
    "        \n",
    "        train_start = process_time_ns()\n",
    "        comb_metrics = []\n",
    "        comb_losses = []\n",
    "        \n",
    "        while not stop_training:\n",
    "            # Entrenamos una vez mas\n",
    "            epochs = epochs + 1\n",
    "            \n",
    "            # Preparamos el optimizador y el scheduler\n",
    "            optimizer = AdamW(\n",
    "                model_copy.parameters(), \n",
    "                lr = learning_rate,\n",
    "                weight_decay = weight_decay,\n",
    "            )\n",
    "\n",
    "            # scheduler = get_inverse_sqrt_schedule(\n",
    "            #    optimizer,\n",
    "            #    num_warmup_steps = 10\n",
    "            #)\n",
    "\n",
    "            scheduler = StepLR(optimizer, step_size=10, gamma=0.9)\n",
    "            \n",
    "            _, _, losses = train_model(model_copy, train_data_loader, loss_fn, optimizer, scheduler, verbose)\n",
    "            \n",
    "            # Medimos\n",
    "            eval_start = process_time_ns()\n",
    "            labels, predictions = eval_model(model_copy, val_data_loader, loss_fn, verbose)\n",
    "            eval_end = process_time_ns()\n",
    "            metrics = compute_metrics(labels, predictions)\n",
    "\n",
    "            print()\n",
    "            print(metrics)\n",
    "            print()\n",
    "            \n",
    "            # Guardamos la informacion\n",
    "            comb_metrics.append(metrics.to_dict())\n",
    "            comb_losses.append(losses)\n",
    "            \n",
    "            # Comprobamos si paramos ya\n",
    "            stop_training = (metrics[\"f1\"].item() - prev_f1 < 0.01)\n",
    "            prev_f1 = metrics[\"f1\"].item()\n",
    "\n",
    "        train_end = process_time_ns()\n",
    "        \n",
    "        df_comb_metrics = pd.DataFrame(comb_metrics)\n",
    "        df_comb_metrics.to_csv(f\"./double_bert_results/metrics_grid-lr_{learning_rate}-wd_{weight_decay}.csv\")\n",
    "        \n",
    "        df_comb_losses = pd.DataFrame(comb_losses)\n",
    "        df_comb_losses.to_csv(f\"./double_bert_results/losses_grid-lr_{learning_rate}-wd_{weight_decay}.csv\")\n",
    "        \n",
    "        metrics[\"train_time_secs\"] = (train_end - train_start) // (10 ** 9)\n",
    "        metrics[\"eval_time_secs\"] = (eval_end - eval_start) // (10 ** 9)\n",
    "        \n",
    "        # Guardamos las medidas\n",
    "        all_combs.append(combination)\n",
    "        all_metrics.append(metrics.to_dict())\n",
    "        all_losses.append(losses)\n",
    "                    \n",
    "        del model_copy\n",
    "        \n",
    "    df_combs = pd.DataFrame(all_combs, index = range(num_combinations), columns=['epochs', 'batch_size', 'learning_rate', 'betas', 'epsilon', 'weight_decay', 'warmup_steps'])\n",
    "    df_metrics = pd.DataFrame(all_metrics)\n",
    "    df_metrics.index = range(num_combinations)\n",
    "    df_results = pd.concat([df_combs, df_metrics], axis=1)\n",
    "    \n",
    "    df_losses = pd.DataFrame(all_losses, index = range(num_combinations))\n",
    "    \n",
    "    return df_results, df_losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"./datasets/database_all_propiedades.csv\").sample(frac=1, random_state=0)\n",
    "df = df.rename(columns = {\n",
    "    \"Sequence\": \"aa_seq\"\n",
    "})\n",
    "df = df.drop(df[df[\"Activity\"] == \"Unknown\"].index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/anaconda3/lib/python3.7/site-packages/pandas/core/indexing.py:1676: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  self._setitem_single_column(ilocs[0], value, pi)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "train_df, test_df, _, _ = train_test_split(\n",
    "    df, \n",
    "    df[\"AMP\"], \n",
    "    test_size=0.2,\n",
    "    random_state=0,\n",
    "    stratify=df[\"AMP\"]\n",
    ")\n",
    "\n",
    "train_df, val_df, _, _ = train_test_split(\n",
    "    train_df, \n",
    "    train_df[\"AMP\"], \n",
    "    test_size=0.2,\n",
    "    random_state=0,\n",
    "    stratify=train_df[\"AMP\"]\n",
    ")\n",
    "\n",
    "biochem_cols = [\n",
    "    \"molecular_mass\",\n",
    "    \"hydrophobic_freq\",\n",
    "    \"hydrophilic_freq\",\n",
    "    \"basic_freq\",\n",
    "    \"acid_freq\",\n",
    "    \"charge\",\n",
    "    \"aliphatic_index\",\n",
    "    \"average_hydrophobicity\",\n",
    "    \"isoelectric_point\"\n",
    "]\n",
    "\n",
    "BATCH_SIZE = 8\n",
    "\n",
    "train_dataloader = AMP_BioChemDataLoader(train_df, biochem_cols, batch_size = BATCH_SIZE)\n",
    "val_dataloader = AMP_BioChemDataLoader(val_df, biochem_cols, batch_size = BATCH_SIZE)\n",
    "test_dataloader = AMP_BioChemDataLoader(test_df, biochem_cols, batch_size = BATCH_SIZE)            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at Rostlab/prot_bert_bfd were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.predictions.decoder.bias']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at Rostlab/prot_bert_bfd and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 158.00 MiB (GPU 0; 7.80 GiB total capacity; 120.00 KiB already allocated; 2.31 MiB free; 2.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOutOfMemoryError\u001b[0m                          Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-fb5efa46cab7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;31m# Copiar el modelo para entrenarlo\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0mbert_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mBertForSequenceClassification\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_pretrained\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Rostlab/prot_bert_bfd'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m \u001b[0mmulti_gpu_bert\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mMultiGPUDoubleBertForPeptideClassification\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbert_model\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbiochem_cols\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0mdf_results\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdf_losses\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgrid_search_early_stop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmulti_gpu_bert\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_dataloader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_dataloader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrid\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mBATCH_SIZE\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-1-ec45bebf0b84>\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, bert_model_for_class, biochem_global_cols)\u001b[0m\n\u001b[1;32m     12\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_labels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbert_model_for_class\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_labels\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraw_seq_bert\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mMultiGPUBertModel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbert_model_for_class\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbert\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraw_seq_dropout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDropout\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0.01\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"cuda:0\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Escritorio/MyGit/AMP-BERT-BIOCHEM/notebooks/MultiGPUModels.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, bert_model)\u001b[0m\n\u001b[1;32m    205\u001b[0m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mMultiGPUBertModel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbert_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    206\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 207\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0membeddings\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbert_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0membeddings\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"cuda:0\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    208\u001b[0m       \u001b[0;31m# self.encoder = bert_model.encoder\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    209\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoder\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mMultiGPUBertEncoder\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbert_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoder\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36mto\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    987\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_floating_point\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_complex\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_blocking\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    988\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 989\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconvert\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    990\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    991\u001b[0m     def register_backward_hook(\n",
      "\u001b[0;32m~/.local/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_apply\u001b[0;34m(self, fn)\u001b[0m\n\u001b[1;32m    639\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    640\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchildren\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 641\u001b[0;31m             \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    642\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    643\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mcompute_should_use_set_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtensor_applied\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_apply\u001b[0;34m(self, fn)\u001b[0m\n\u001b[1;32m    662\u001b[0m             \u001b[0;31m# `with torch.no_grad():`\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    663\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 664\u001b[0;31m                 \u001b[0mparam_applied\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparam\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    665\u001b[0m             \u001b[0mshould_use_set_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompute_should_use_set_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparam\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparam_applied\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    666\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mshould_use_set_data\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.local/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36mconvert\u001b[0;34m(t)\u001b[0m\n\u001b[1;32m    985\u001b[0m                 return t.to(device, dtype if t.is_floating_point() or t.is_complex() else None,\n\u001b[1;32m    986\u001b[0m                             non_blocking, memory_format=convert_to_format)\n\u001b[0;32m--> 987\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_floating_point\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_complex\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_blocking\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    988\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    989\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_apply\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mconvert\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mOutOfMemoryError\u001b[0m: CUDA out of memory. Tried to allocate 158.00 MiB (GPU 0; 7.80 GiB total capacity; 120.00 KiB already allocated; 2.31 MiB free; 2.00 MiB reserved in total by PyTorch) If reserved memory is >> allocated memory try setting max_split_size_mb to avoid fragmentation.  See documentation for Memory Management and PYTORCH_CUDA_ALLOC_CONF"
     ]
    }
   ],
   "source": [
    "from pipeline_tools import train_model, eval_model, compute_metrics\n",
    "from torch.optim import AdamW\n",
    "from transformers import BertForSequenceClassification, get_linear_schedule_with_warmup\n",
    "from torch.nn import CrossEntropyLoss\n",
    "import pandas as pd\n",
    "\n",
    "grid = {\n",
    "    \"learning_rate\": [5e-5,3e-5,1e-5],\n",
    "    \"weight_decay\": [0.01, 0.05, 0.1]\n",
    "}\n",
    "\n",
    "# Copiar el modelo para entrenarlo\n",
    "bert_model = BertForSequenceClassification.from_pretrained('Rostlab/prot_bert_bfd')   \n",
    "multi_gpu_bert = MultiGPUDoubleBertForPeptideClassification(bert_model, biochem_cols)\n",
    "            \n",
    "df_results, df_losses = grid_search_early_stop(multi_gpu_bert, train_dataloader, val_dataloader, grid, BATCH_SIZE, verbose = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

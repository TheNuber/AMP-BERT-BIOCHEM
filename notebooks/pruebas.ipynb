{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation dataframe: 413 positives, 382 negatives (0.519496855345912%)\n",
      "Test dataframe: 1652 positives, 1526 negatives (0.5198237885462555%)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "SEED = 0\n",
    "FRAC_VAL = 0.2\n",
    "\n",
    "train_df = pd.read_csv('./datasets/all_veltri_2.csv', index_col = 0).sample(frac=1, random_state=SEED)\n",
    "df_pos = pd.read_csv('./datasets/veltri_dramp_cdhit_90_2.csv', index_col = 0)\n",
    "df_neg = pd.read_csv('./datasets/non_amp_ampep_cdhit90_2.csv', index_col = 0)\n",
    "\n",
    "val_df_pos = df_pos.sample(frac=FRAC_VAL, random_state=SEED)\n",
    "val_df_neg = df_neg.sample(frac=FRAC_VAL, random_state=SEED)\n",
    "test_df_pos = df_pos.drop(val_df_pos.index)\n",
    "test_df_neg = df_neg.drop(val_df_neg.index)\n",
    "\n",
    "val_df = pd.concat([val_df_pos, val_df_neg]).sample(frac=1, random_state=SEED)\n",
    "test_df = pd.concat([test_df_pos, test_df_neg]).sample(frac=1, random_state=SEED)\n",
    "\n",
    "print(f\"Validation dataframe: {len(val_df_pos)} positives, {len(val_df_neg)} negatives ({len(val_df_pos)/len(val_df)}%)\")\n",
    "print(f\"Test dataframe: {len(test_df_pos)} positives, {len(test_df_neg)} negatives ({len(test_df_pos)/len(test_df)}%)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'idx': tensor([ 597,  876, 2267, 1412, 1495, 1919, 1514, 1635]), 'input_ids': tensor([[ 2, 10, 15,  ...,  0,  0,  0],\n",
      "        [ 2, 17, 19,  ...,  0,  0,  0],\n",
      "        [ 2, 18, 13,  ...,  0,  0,  0],\n",
      "        ...,\n",
      "        [ 2,  5, 10,  ...,  0,  0,  0],\n",
      "        [ 2, 21, 13,  ...,  0,  0,  0],\n",
      "        [ 2, 21,  6,  ...,  0,  0,  0]]), 'attention_mask': tensor([[1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        ...,\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0],\n",
      "        [1, 1, 1,  ..., 0, 0, 0]]), 'labels': tensor([1, 0, 0, 1, 0, 1, 0, 0]), 'biochem_global_info': tensor([[ 0.6875,  0.4100,  0.5900,  0.1500,  0.0900,  3.0000,  0.2500,  1.1900,\n",
      "          8.2527],\n",
      "        [ 0.1734,  0.4600,  0.5400,  0.0800,  0.2300, -2.0000,  0.0800, -0.6700,\n",
      "          4.0500],\n",
      "        [ 0.3977,  0.3300,  0.6700,  0.2000,  0.1700,  1.0000,  0.1800,  0.6500,\n",
      "          6.0221],\n",
      "        [ 0.7410,  0.2900,  0.7100,  0.1200,  0.1200,  0.0000,  0.1200, -0.6800,\n",
      "          5.6020],\n",
      "        [ 0.3004,  0.5200,  0.4800,  0.0400,  0.0400,  0.0000,  0.3500,  2.5800,\n",
      "          6.0374],\n",
      "        [ 0.2754,  0.4000,  0.6000,  0.3000,  0.1000,  4.0000,  0.2300, -2.3200,\n",
      "          7.0972],\n",
      "        [ 0.7623,  0.4400,  0.5600,  0.1700,  0.1200,  3.0000,  0.3000,  0.6900,\n",
      "          8.3975],\n",
      "        [ 0.6896,  0.4700,  0.5300,  0.3700,  0.0000, 19.0000,  0.2900, -9.1700,\n",
      "         12.0000]]), 'aa_encoding': tensor([[[  0.,   0.,   0.,  ...,   0.,   0.,   0.],\n",
      "         [  0.,   0.,   0.,  ...,  20.,  10.,   0.],\n",
      "         [  0.,   1.,   0.,  ...,   0.,   0.,   0.],\n",
      "         ...,\n",
      "         [  0.,   0.,   0.,  ...,   0.,   0.,   0.],\n",
      "         [  0.,   0.,   0.,  ...,   0.,   0.,   0.],\n",
      "         [  0.,   0.,   0.,  ...,   0.,   0.,   0.]],\n",
      "\n",
      "        [[  0.,   0.,   0.,  ...,   0.,   0.,   0.],\n",
      "         [  0.,   0.,   0.,  ...,  20.,  10.,   0.],\n",
      "         [  0.,   0.,   1.,  ...,   0.,   0.,   0.],\n",
      "         ...,\n",
      "         [  0.,   0.,   0.,  ...,   0.,   0.,   0.],\n",
      "         [  0.,   0.,   0.,  ...,   0.,   0.,   0.],\n",
      "         [  0.,   0.,   0.,  ...,   0.,   0.,   0.]],\n",
      "\n",
      "        [[  0.,   0.,   0.,  ...,  50.,  40.,  20.],\n",
      "         [  0.,   0.,   0.,  ...,   0.,   0.,   0.],\n",
      "         [  0.,   0.,   0.,  ...,   0.,   0.,   0.],\n",
      "         ...,\n",
      "         [  0.,   0.,   0.,  ...,   0.,   0.,   0.],\n",
      "         [  0.,   0.,   0.,  ...,   0.,   0.,   0.],\n",
      "         [  0.,   0.,   0.,  ...,   0.,   0.,   0.]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[  0.,   0.,   0.,  ...,   0.,   0.,   0.],\n",
      "         [  0.,   0.,   0.,  ...,   0.,   0.,   0.],\n",
      "         [  0.,   0.,   0.,  ...,   0.,   0.,   0.],\n",
      "         ...,\n",
      "         [  0.,   0.,   0.,  ...,   0.,   0.,   0.],\n",
      "         [  0.,   0.,   0.,  ...,   0.,   0.,   0.],\n",
      "         [  0.,   0.,   0.,  ...,   0.,   0.,   0.]],\n",
      "\n",
      "        [[  0.,   0.,   0.,  ..., -10.,  -5.,   0.],\n",
      "         [  0.,   0.,   0.,  ...,   0.,   0.,   0.],\n",
      "         [  1.,   0.,   0.,  ...,   0.,   0.,   0.],\n",
      "         ...,\n",
      "         [  0.,   0.,   0.,  ...,   0.,   0.,   0.],\n",
      "         [  0.,   0.,   0.,  ...,   0.,   0.,   0.],\n",
      "         [  0.,   0.,   0.,  ...,   0.,   0.,   0.]],\n",
      "\n",
      "        [[  0.,   0.,   0.,  ..., -10.,  -5.,   0.],\n",
      "         [  1.,   0.,   0.,  ...,   0.,   0.,   0.],\n",
      "         [  0.,   0.,   0.,  ...,   0.,   0.,   0.],\n",
      "         ...,\n",
      "         [  0.,   0.,   0.,  ...,   0.,   0.,   0.],\n",
      "         [  0.,   0.,   0.,  ...,   0.,   0.,   0.],\n",
      "         [  0.,   0.,   0.,  ...,   0.,   0.,   0.]]])}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at Rostlab/prot_bert_bfd were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.bias', 'cls.predictions.decoder.bias', 'cls.seq_relationship.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at Rostlab/prot_bert_bfd and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 10/445: Loss (avg) 0.45909324256595474, Step Time 742 ms, ETA 5:22\n",
      "Step 20/445: Loss (avg) 0.6141726576076305, Step Time 741 ms, ETA 5:14\n",
      "Step 30/445: Loss (avg) 0.6720002518053636, Step Time 745 ms, ETA 5:9\n",
      "Step 40/445: Loss (avg) 0.683722405948407, Step Time 744 ms, ETA 5:1\n",
      "Step 50/445: Loss (avg) 0.665407795332585, Step Time 748 ms, ETA 4:55\n",
      "Step 60/445: Loss (avg) 0.6280311669210209, Step Time 750 ms, ETA 4:48\n",
      "Step 70/445: Loss (avg) 0.5897950519040858, Step Time 749 ms, ETA 4:40\n",
      "Step 80/445: Loss (avg) 0.569161513595918, Step Time 753 ms, ETA 4:34\n",
      "Step 90/445: Loss (avg) 0.5177453564756024, Step Time 751 ms, ETA 4:26\n",
      "Step 100/445: Loss (avg) 0.5384012018354685, Step Time 750 ms, ETA 4:18\n",
      "Step 110/445: Loss (avg) 0.5182216736591545, Step Time 750 ms, ETA 4:11\n",
      "Step 120/445: Loss (avg) 0.45306408156974337, Step Time 752 ms, ETA 4:4\n",
      "Step 130/445: Loss (avg) 0.4502880856582586, Step Time 755 ms, ETA 3:57\n",
      "Step 140/445: Loss (avg) 0.4452466773800974, Step Time 754 ms, ETA 3:49\n",
      "Step 150/445: Loss (avg) 0.39159591206914324, Step Time 752 ms, ETA 3:41\n",
      "Step 160/445: Loss (avg) 0.35925662131377534, Step Time 756 ms, ETA 3:35\n",
      "Step 170/445: Loss (avg) 0.36386850837653145, Step Time 754 ms, ETA 3:27\n",
      "Step 180/445: Loss (avg) 0.3731417304119191, Step Time 756 ms, ETA 3:20\n",
      "Step 190/445: Loss (avg) 0.3406825103996372, Step Time 757 ms, ETA 3:13\n",
      "Step 200/445: Loss (avg) 0.3186623747284222, Step Time 755 ms, ETA 3:4\n",
      "Step 210/445: Loss (avg) 0.3773383043993417, Step Time 749 ms, ETA 2:56\n",
      "Step 220/445: Loss (avg) 0.35735156804719903, Step Time 755 ms, ETA 2:49\n",
      "Step 230/445: Loss (avg) 0.33710860351727123, Step Time 757 ms, ETA 2:42\n",
      "Step 240/445: Loss (avg) 0.35631290954046235, Step Time 755 ms, ETA 2:34\n",
      "Step 250/445: Loss (avg) 0.37784880683914224, Step Time 755 ms, ETA 2:27\n",
      "Step 260/445: Loss (avg) 0.3221916845637372, Step Time 754 ms, ETA 2:19\n",
      "Step 270/445: Loss (avg) 0.3486781828251396, Step Time 758 ms, ETA 2:12\n",
      "Step 280/445: Loss (avg) 0.3255638298513534, Step Time 757 ms, ETA 2:4\n",
      "Step 290/445: Loss (avg) 0.29765841014394895, Step Time 751 ms, ETA 1:56\n",
      "Step 300/445: Loss (avg) 0.27846711294418464, Step Time 758 ms, ETA 1:49\n",
      "Step 310/445: Loss (avg) 0.27272006857237524, Step Time 760 ms, ETA 1:42\n",
      "Step 320/445: Loss (avg) 0.24914974110645008, Step Time 759 ms, ETA 1:34\n",
      "Step 330/445: Loss (avg) 0.28029343471051416, Step Time 757 ms, ETA 1:27\n",
      "Step 340/445: Loss (avg) 0.32347620562317486, Step Time 754 ms, ETA 1:19\n",
      "Step 350/445: Loss (avg) 0.2483809486830205, Step Time 760 ms, ETA 1:12\n",
      "Step 360/445: Loss (avg) 0.234383964327589, Step Time 758 ms, ETA 1:4\n",
      "Step 370/445: Loss (avg) 0.20555492494295097, Step Time 759 ms, ETA 0:56\n",
      "Step 380/445: Loss (avg) 0.22299892890765255, Step Time 758 ms, ETA 0:49\n",
      "Step 390/445: Loss (avg) 0.2596266048036735, Step Time 751 ms, ETA 0:41\n",
      "Step 400/445: Loss (avg) 0.24730289562330235, Step Time 761 ms, ETA 0:34\n",
      "Step 410/445: Loss (avg) 0.1821777219758243, Step Time 752 ms, ETA 0:26\n",
      "Step 420/445: Loss (avg) 0.23648100895115365, Step Time 760 ms, ETA 0:19\n",
      "Step 430/445: Loss (avg) 0.21644971390679807, Step Time 758 ms, ETA 0:11\n",
      "Step 440/445: Loss (avg) 0.24991143754705542, Step Time 757 ms, ETA 0:3\n",
      "Metrics for train set: \n",
      "   accuracy        f1  precision    recall  specificity     auroc      aupr  \\\n",
      "0  0.864736  0.856031   0.914907  0.804274     0.925197  0.864736  0.908522   \n",
      "\n",
      "             confusion_matrix  \n",
      "0  [[1645, 133], [348, 1430]]  \n",
      "Step 10/398: Loss (avg) 0.3133023731140511, Step Time 238 ms, ETA 1:32\n",
      "Step 20/398: Loss (avg) 0.41743679553174984, Step Time 240 ms, ETA 1:30\n",
      "Step 30/398: Loss (avg) 0.5937845712995892, Step Time 239 ms, ETA 1:27\n",
      "Step 40/398: Loss (avg) 0.613155270077473, Step Time 239 ms, ETA 1:25\n",
      "Step 50/398: Loss (avg) 0.5149569720988727, Step Time 240 ms, ETA 1:23\n",
      "Step 60/398: Loss (avg) 0.5187514906388105, Step Time 240 ms, ETA 1:21\n",
      "Step 70/398: Loss (avg) 0.4983045244206692, Step Time 240 ms, ETA 1:18\n",
      "Step 80/398: Loss (avg) 0.43529759916340394, Step Time 240 ms, ETA 1:16\n",
      "Step 90/398: Loss (avg) 0.4524539275832805, Step Time 239 ms, ETA 1:13\n",
      "Step 100/398: Loss (avg) 0.45339011832669013, Step Time 240 ms, ETA 1:11\n",
      "Step 110/398: Loss (avg) 0.448421978289782, Step Time 239 ms, ETA 1:8\n",
      "Step 120/398: Loss (avg) 0.5208302219789669, Step Time 240 ms, ETA 1:6\n",
      "Step 130/398: Loss (avg) 0.5273619411223844, Step Time 240 ms, ETA 1:4\n",
      "Step 140/398: Loss (avg) 0.48119184617452876, Step Time 239 ms, ETA 1:1\n",
      "Step 150/398: Loss (avg) 0.5501009926709561, Step Time 239 ms, ETA 0:59\n",
      "Step 160/398: Loss (avg) 0.5281340199017446, Step Time 239 ms, ETA 0:56\n",
      "Step 170/398: Loss (avg) 0.494982613904204, Step Time 240 ms, ETA 0:54\n",
      "Step 180/398: Loss (avg) 0.5556141139937489, Step Time 240 ms, ETA 0:52\n",
      "Step 190/398: Loss (avg) 0.5740458605167541, Step Time 240 ms, ETA 0:49\n",
      "Step 200/398: Loss (avg) 0.4791985253167495, Step Time 239 ms, ETA 0:47\n",
      "Step 210/398: Loss (avg) 0.5667181811612368, Step Time 240 ms, ETA 0:45\n",
      "Step 220/398: Loss (avg) 0.5594221796329234, Step Time 240 ms, ETA 0:42\n",
      "Step 230/398: Loss (avg) 0.5454100870620373, Step Time 240 ms, ETA 0:40\n",
      "Step 240/398: Loss (avg) 0.5469351916437013, Step Time 239 ms, ETA 0:37\n",
      "Step 250/398: Loss (avg) 0.6247174518620945, Step Time 240 ms, ETA 0:35\n",
      "Step 260/398: Loss (avg) 0.5780672773823226, Step Time 239 ms, ETA 0:32\n",
      "Step 270/398: Loss (avg) 0.5429417808910748, Step Time 239 ms, ETA 0:30\n",
      "Step 280/398: Loss (avg) 0.6861523274215786, Step Time 240 ms, ETA 0:28\n",
      "Step 290/398: Loss (avg) 0.6729418925777388, Step Time 239 ms, ETA 0:25\n",
      "Step 300/398: Loss (avg) 0.664394620813769, Step Time 239 ms, ETA 0:23\n",
      "Step 310/398: Loss (avg) 0.5231631612563103, Step Time 240 ms, ETA 0:21\n",
      "Step 320/398: Loss (avg) 0.5231818875168164, Step Time 240 ms, ETA 0:18\n",
      "Step 330/398: Loss (avg) 0.6349903408887647, Step Time 239 ms, ETA 0:16\n",
      "Step 340/398: Loss (avg) 0.4903078751384276, Step Time 239 ms, ETA 0:13\n",
      "Step 350/398: Loss (avg) 0.6343341938403859, Step Time 239 ms, ETA 0:11\n",
      "Step 360/398: Loss (avg) 0.4987279588451985, Step Time 239 ms, ETA 0:9\n",
      "Step 370/398: Loss (avg) 0.5252190217976581, Step Time 240 ms, ETA 0:6\n",
      "Step 380/398: Loss (avg) 0.5158357627612238, Step Time 240 ms, ETA 0:4\n",
      "Step 390/398: Loss (avg) 0.5434193359868158, Step Time 240 ms, ETA 0:1\n",
      "Metrics for test set: \n",
      "   accuracy        f1  precision    recall  specificity     auroc     aupr  \\\n",
      "0  0.787602  0.806977    0.76477  0.854116     0.715596  0.784856  0.84736   \n",
      "\n",
      "             confusion_matrix  \n",
      "0  [[1092, 434], [241, 1411]]  \n"
     ]
    }
   ],
   "source": [
    "from pipeline_tools import train_model, eval_model, compute_metrics\n",
    "from torch.optim import AdamW\n",
    "from transformers import BertForSequenceClassification, get_linear_schedule_with_warmup\n",
    "from torch.nn import CrossEntropyLoss\n",
    "\n",
    "BATCH_SIZE = 8\n",
    "LEARNING_RATE = 5e-6\n",
    "WEIGHT_DECAY = 0.01\n",
    "EPOCHS = 1\n",
    "            \n",
    "biochem_cols = [\n",
    "    \"molecular_mass\",\n",
    "    \"hydrophobic_freq\",\n",
    "    \"hydrophilic_freq\",\n",
    "    \"basic_freq\",\n",
    "    \"acid_freq\",\n",
    "    \"charge\",\n",
    "    \"aliphatic_index\",\n",
    "    \"average_hydrophobicity\",\n",
    "    \"isoelectric_point\"\n",
    "]\n",
    "\n",
    "train_dataloader = AMP_BioChemDataLoader(train_df, biochem_cols, batch_size = BATCH_SIZE)            \n",
    "test_dataloader = AMP_BioChemDataLoader(test_df, biochem_cols, batch_size = BATCH_SIZE)            \n",
    "\n",
    "# Copiar el modelo para entrenarlo\n",
    "bert_model = BertForSequenceClassification.from_pretrained('Rostlab/prot_bert_bfd')   \n",
    "multi_gpu_bert = MultiGPUDoubleBertForPeptideClassification(bert_model, biochem_cols)\n",
    "            \n",
    "# Entrenar el modelo con esta configuracion\n",
    "optimizer = AdamW(\n",
    "    multi_gpu_bert.parameters(), \n",
    "    lr = LEARNING_RATE, \n",
    "    weight_decay = WEIGHT_DECAY)\n",
    "            \n",
    "total_steps = len(train_dataloader) * EPOCHS\n",
    "            \n",
    "scheduler = get_linear_schedule_with_warmup(optimizer, \n",
    "    num_warmup_steps = 0, \n",
    "    num_training_steps = total_steps)\n",
    "\n",
    "for i in range(EPOCHS):\n",
    "    labels, predicted, _ = train_model(multi_gpu_bert, train_dataloader, CrossEntropyLoss(), optimizer, scheduler, True)\n",
    "\n",
    "# Obtener las métricas de entrenamiento\n",
    "train_metrics = compute_metrics(labels, predicted)\n",
    "            \n",
    "print(f\"Metrics for train set: \")\n",
    "print(train_metrics)\n",
    "\n",
    "# Obtener las métricas de validacion\n",
    "test_labels, test_preds = eval_model(multi_gpu_bert, test_dataloader, CrossEntropyLoss(), True)\n",
    "test_metrics = compute_metrics(test_labels, test_preds)\n",
    "            \n",
    "print(f\"Metrics for test set: \")\n",
    "print(test_metrics)\n",
    "\n",
    "train_metrics.to_csv('./biochem_results/train_metrics_with_biochem_LL_1ep.csv')\n",
    "test_metrics.to_csv('./biochem_results/test_metrics_with_biochem_LL_1ep.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
